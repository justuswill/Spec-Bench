model:
  _target_: model.ptp.lit.ParallelSamplingLightningModule
  loss_type: bce
  error_correction: true
  tokens_to_fill: 1024
  tokens_per_student_call: 20
  student_calls_per_step: 1
  student:
    _target_: model.ptp.transformer.MixedTransformerModel
    # model_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
    model_id: lmsys/vicuna-7b-v1.5
    #  _target_: model.ptp.scaling.make_scaling_llama
    #  config_name: 34M
    shift_positions: true
    gated_lora: true
    # merge: true
    lora_config:
      r: 128
      target_modules:
        - q_proj
        - k_proj
        - v_proj
        - o_proj
        - gate_proj
        - up_proj
        - down_proj
  # teacher:
  #  _target_: model.ptp.transformer.TransformerModel
  #  model_id: lmsys/vicuna-7b-v1.5
  temperature: 0.7
  top_k: 40